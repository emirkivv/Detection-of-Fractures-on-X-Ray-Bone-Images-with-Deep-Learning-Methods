# -*- coding: utf-8 -*-
"""Faster R-CNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JzKzCGCjEGtK0HOu1f-MaBb8W6tL--U6
"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import os

os.environ['KAGGLE_USERNAME'] = 'username'
os.environ['KAGGLE_KEY'] = 'key'

!pip install kaggle

!kaggle datasets download emirkiv/fracture-detection-ekiha-final
!unzip fracture-detection-ekiha-final.zip > /dev/null

import os
train_images = '/content/431/images'
train_labels = '/content/431/labels'

print('Number of total images: ' + str(len(os.listdir(train_images))))
print('Number of total labels: ' + str(len(os.listdir(train_labels))))

import os
train_images = '/content/dataset_final/2_dataset_no_aug_mixed/train/images'
train_labels = '/content/dataset_final/2_dataset_no_aug_mixed/train/labels'

test_images = '/content/dataset_final/2_dataset_no_aug_mixed/test/images'
test_labels = '/content/dataset_final/2_dataset_no_aug_mixed/test/labels'

val_images = '/content/dataset_final/2_dataset_no_aug_mixed/valid/images'
val_labels = '/content/dataset_final/2_dataset_no_aug_mixed/valid/labels'

print('Number of train images: ' + str(len(os.listdir(train_images))))
print('Number of train labels: ' + str(len(os.listdir(train_labels))))
print('Number of val images: ' + str(len(os.listdir(val_images))))
print('Number of val labels: ' + str(len(os.listdir(val_labels))))
print('Number of test images: ' + str(len(os.listdir(test_images))))
print('Number of test labels: ' + str(len(os.listdir(test_labels))))
print('Total images: ' + str(len(os.listdir(train_images)) + len(os.listdir(test_images)) + len(os.listdir(val_images))))

!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

import os
import cv2
import torch
import numpy as np
from pathlib import Path
from functools import partial
from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.structures import BoxMode

class Config:
    PATHS = {
        "train_images": "/content/dataset_final/2_dataset_no_aug_mixed/train/images",
        "train_labels": "/content/dataset_final/2_dataset_no_aug_mixed/train/labels",
        "val_images": "/content/dataset_final/2_dataset_no_aug_mixed/valid/images",
        "val_labels": "/content/dataset_final/2_dataset_no_aug_mixed/valid/labels",
        "test_images": "/content/dataset_final/2_dataset_no_aug_mixed/test/images",
        "test_labels": "/content/dataset_final/2_dataset_no_aug_mixed/test/labels",
        "output": "/content/output"
    }
    CLASSES = ["Fracture"]
    MODEL = "COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml"

# Annonations YOLO to Detectron2
def get_fracture_dicts(img_dir, label_dir):
    dataset_dicts = []
    img_dir = Path(img_dir)
    label_dir = Path(label_dir)

    for img_path in img_dir.glob("*.[jJpP][pPnN][gG]"):
        label_path = label_dir / f"{img_path.stem}.txt"
        if not label_path.exists():
            continue

        image = cv2.imread(str(img_path))
        height, width = image.shape[:2]

        record = {
            "file_name": str(img_path),
            "image_id": img_path.stem,
            "height": height,
            "width": width,
            "annotations": []
        }

        with open(label_path) as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) != 5:
                    continue

                class_id, x_center, y_center, w, h = map(float, parts)

                x_center_abs = x_center * width
                y_center_abs = y_center * height
                w_abs = w * width
                h_abs = h * height

                record["annotations"].append({
                    "bbox": [
                        x_center_abs - w_abs/2,  # x_min
                        y_center_abs - h_abs/2,  # y_min
                        w_abs,                   # width
                        h_abs                    # height
                    ],
                    "bbox_mode": BoxMode.XYWH_ABS,
                    "category_id": int(class_id),
                })

        if record["annotations"]:
            dataset_dicts.append(record)

    print(f"Loaded {len(dataset_dicts)} images from {img_dir}")
    return dataset_dicts

# Dataset Registration
def clean_registrations():
    for split in ["train", "val", "test"]:
        name = f"fracture_{split}"
        if name in DatasetCatalog.list():
            DatasetCatalog.remove(name)
        if name in MetadataCatalog.list():
            MetadataCatalog.remove(name)
        print(f"Cleared registration for {name}")

clean_registrations()

for split in ["train", "val", "test"]:
    DatasetCatalog.register(
        f"fracture_{split}",
        partial(get_fracture_dicts,
                Config.PATHS[f"{split}_images"],
                Config.PATHS[f"{split}_labels"])
    )
    MetadataCatalog.get(f"fracture_{split}").set(thing_classes=Config.CLASSES)
    print(f"Registered fracture_{split}")

# Verify dataset registration
print("\nRegistered datasets:", DatasetCatalog.list())
assert "fracture_train" in DatasetCatalog.list(), "Train dataset not registered!"
assert "fracture_val" in DatasetCatalog.list(), "Validation dataset not registered!"
assert "fracture_test" in DatasetCatalog.list(), "Test dataset not registered!"

# Model Configuration
cfg = get_cfg()

# Output Directory
Path(Config.PATHS["output"]).mkdir(parents=True, exist_ok=True)

try:
    cfg.merge_from_file(model_zoo.get_config_file(Config.MODEL))
except RuntimeError as e:
    print(f"Error loading model: {e}")

# Model Hyperparameters
cfg.DATASETS.TRAIN = ("fracture_train",)
cfg.DATASETS.TEST = ("fracture_test",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(Config.MODEL)
cfg.SOLVER.IMS_PER_BATCH = 16  # Batch size, 8-16
cfg.SOLVER.BASE_LR = 0.001 # Learning rate
cfg.SOLVER.MAX_ITER = 2275    # Max iterasyon = (Train görüntü sayısı * Epoch) / (Batch size)
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64  # 32-64 sistem gereksinimini azaltır ama performans düşer, 96-128-256 sistem gereksinimi artar performans artar
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(Config.CLASSES)
cfg.SOLVER.CHECKPOINT_PERIOD = 2276


cfg.OUTPUT_DIR = Config.PATHS["output"]

# Training with Verification
print(f"\nCUDA available: {torch.cuda.is_available()}")
print(f"Using {torch.cuda.device_count()} GPU(s)")

# Verify metadata
train_metadata = MetadataCatalog.get("fracture_train")
print("\nTraining classes:", train_metadata.thing_classes)

# Initialize trainer
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)

print("\nStarting training...")
trainer.train()

# Final Output
print(f"\nTraining complete! Model saved to: {cfg.OUTPUT_DIR}")

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Load trained model
cfg.MODEL.WEIGHTS = str(Path(cfg.OUTPUT_DIR) / "model_final.pth")


predictor = DefaultPredictor(cfg)

# Load test dataset
val_loader = build_detection_test_loader(cfg, "fracture_test")
evaluator = COCOEvaluator("fracture_test", cfg, False, output_dir=cfg.OUTPUT_DIR)
results = inference_on_dataset(predictor.model, val_loader, evaluator)

# Extract precision, recall, F1-score
precision = results["bbox"]["AP"] / 100  # Convert to 0-1 scale
recall = results["bbox"]["AP50"] / 100
f1 = 2 * (precision * recall) / (precision + recall)

print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}")

import torch
from detectron2.engine import DefaultPredictor
from detectron2.evaluation import COCOEvaluator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from collections import defaultdict

def calculate_iou(box1, box2):
    """Calculate IoU between two boxes in XYWH format"""
    # Convert to xyxy format
    box1_xyxy = [box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3]]
    box2_xyxy = [box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3]]

    # Calculate intersection
    x1 = max(box1_xyxy[0], box2_xyxy[0])
    y1 = max(box1_xyxy[1], box2_xyxy[1])
    x2 = min(box1_xyxy[2], box2_xyxy[2])
    y2 = min(box1_xyxy[3], box2_xyxy[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)

    # Calculate union
    box1_area = box1[2] * box1[3]
    box2_area = box2[2] * box2[3]
    union = box1_area + box2_area - intersection

    return intersection / union if union > 0 else 0

def evaluate_detections(predictor, dataset_name, iou_threshold=0.5, conf_threshold=0.5):
    """
    Evaluate object detection results and calculate confusion matrix metrics
    """
    dataset_dicts = DatasetCatalog.get(dataset_name)

    total_gt = 0
    tp = 0
    fp = 0
    fn = 0

    # Store confidences and true/false positives for PR curve
    all_confidences = []
    all_true_positives = []

    print(f"Evaluating {len(dataset_dicts)} images...")

    for d in tqdm(dataset_dicts):
        # Get ground truth boxes
        gt_boxes = [ann["bbox"] for ann in d["annotations"]]
        total_gt += len(gt_boxes)

        # Get predictions
        img = cv2.imread(d["file_name"])
        outputs = predictor(img)
        pred_boxes = outputs["instances"].pred_boxes.tensor.cpu().numpy()
        scores = outputs["instances"].scores.cpu().numpy()

        # Filter predictions by confidence threshold
        mask = scores >= conf_threshold
        pred_boxes = pred_boxes[mask]
        scores = scores[mask]

        # Match predictions to ground truth
        matched_gt = set()

        for pred_idx, pred_box in enumerate(pred_boxes):
            best_iou = 0
            best_gt_idx = -1

            # Convert pred_box from xyxy to xywh format
            pred_box_xywh = [
                pred_box[0],
                pred_box[1],
                pred_box[2] - pred_box[0],
                pred_box[3] - pred_box[1]
            ]

            # Find best matching ground truth box
            for gt_idx, gt_box in enumerate(gt_boxes):
                if gt_idx in matched_gt:
                    continue

                iou = calculate_iou(pred_box_xywh, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx

            # Store confidence and whether it's a true positive
            all_confidences.append(scores[pred_idx])

            if best_iou >= iou_threshold:
                tp += 1
                matched_gt.add(best_gt_idx)
                all_true_positives.append(1)
            else:
                fp += 1
                all_true_positives.append(0)

        # Count unmatched ground truth boxes as false negatives
        fn += len(gt_boxes) - len(matched_gt)

    # Calculate precision, recall, and F1 score
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    # Create confusion matrix visualization
    cm = np.array([[tp, fn],
               [fp, 0]])

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Positive', 'Negative'],
            yticklabels=['Positive', 'Negative'])
    plt.title(f'Confusion Matrix (IoU >= {iou_threshold}, Conf >= {conf_threshold})')
    plt.ylabel('Ground Truth')
    plt.xlabel('Predicted')
    plt.savefig(f'{cfg.OUTPUT_DIR}/confusion_matrix.png')
    #plt.close()


    # Print metrics
    print("\nDetection Metrics:")
    print(f"True Positives (TP): {tp}")
    print(f"False Positives (FP): {fp}")
    print(f"False Negatives (FN): {fn}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    return {
        'tp': tp,
        'fp': fp,
        'fn': fn,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Create predictor from trained model
predictor = DefaultPredictor(cfg)

# Evaluate on validation set
print("Evaluating model performance...")
metrics = evaluate_detections(
    predictor,
    "fracture_test", #Performans ölçütü alınacak olan dataset burada belirtilir
    iou_threshold=0.5,  # Standard COCO IoU threshold = 0.5
    conf_threshold=0.5  # Confidence threshold for predictions = standard 0.5
)

# Save metrics to file
with open(f'{cfg.OUTPUT_DIR}/evaluation_metrics.txt', 'w') as f:
    f.write("Object Detection Evaluation Metrics\n")
    f.write("=================================\n")
    f.write(f"True Positives (TP): {metrics['tp']}\n")
    f.write(f"False Positives (FP): {metrics['fp']}\n")
    f.write(f"False Negatives (FN): {metrics['fn']}\n")
    f.write(f"Precision: {metrics['precision']:.4f}\n")
    f.write(f"Recall: {metrics['recall']:.4f}\n")
    f.write(f"F1 Score: {metrics['f1']:.4f}\n")

# Ground Truth vs Model Output
import cv2
import torch
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog, DatasetCatalog
from IPython.display import display, Markdown, Image

# Configuration class with visualization settings
class PredictionConfig:
    MODEL_PATH = "/content/output/model_final.pth"
    SCORE_THRESH = 0.7
    CLASSES = ["Fracture"]
    CLASS_COLORS = [
        (0, 255, 255)
    ]
    DISPLAY_WIDTH = 1200
    DISPLAY_HEIGHT = 800

# Model configuration setup
def setup_cfg():
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml"))
    cfg.MODEL.WEIGHTS = PredictionConfig.MODEL_PATH
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = PredictionConfig.SCORE_THRESH
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(PredictionConfig.CLASSES)
    cfg.MODEL.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    return cfg

# Initialize metadata with custom colors
def setup_metadata():
    if "fracture_train" in MetadataCatalog.list():
        MetadataCatalog.remove("fracture_train")

    metadata = MetadataCatalog.get("fracture_train")
    metadata.set(
        thing_classes=PredictionConfig.CLASSES,
        thing_colors=PredictionConfig.CLASS_COLORS
    )
    return metadata

# Initialize predictor and metadata
cfg = setup_cfg()
metadata = setup_metadata()
predictor = DefaultPredictor(cfg)

# Function to read YOLO format labels
def read_yolo_labels(label_path, img_width, img_height):
    with open(label_path, "r") as f:
        lines = f.readlines()

    boxes = []
    for line in lines:
        parts = line.strip().split()
        x_center, y_center, w, h = map(float, parts[1:])

        x_min = int((x_center - w / 2) * img_width)
        y_min = int((y_center - h / 2) * img_height)
        x_max = int((x_center + w / 2) * img_width)
        y_max = int((y_center + h / 2) * img_height)

        boxes.append((x_min, y_min, x_max, y_max))

    return boxes

# Function to display ground truth and model output side by side
def predict_and_display(image_path, label_path, output_path=None):
    try:
        image = cv2.imread(image_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {image_path}")
    except Exception as e:
        print(f"Error loading image: {str(e)}")
        return None

    img_height, img_width = image.shape[:2]
    gt_boxes = read_yolo_labels(label_path, img_width, img_height)

    outputs = predictor(image)
    v = Visualizer(
        image[:, :, ::-1],
        metadata=metadata,
        scale=1.0,
        instance_mode=ColorMode.SEGMENTATION
    )
    vis_output = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    vis_image = vis_output.get_image()

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    for (x_min, y_min, x_max, y_max) in gt_boxes:
        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)

    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    axes[0].set_title("Ground Truth")
    axes[0].axis("off")

    axes[1].imshow(vis_image)
    axes[1].set_title("Model Output")
    axes[1].axis("off")

    if output_path:
        plt.savefig(output_path, bbox_inches='tight', pad_inches=0.1)
        print(f"✅ Saved result to: {output_path}")
        plt.show()
        plt.close()
    else:
        plt.show()
        plt.close()

    return outputs

# Batch processing with display support
def process_directory(image_dir, label_dir, output_dir):
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    image_paths = sorted(Path(image_dir).glob("*.[jJpP][pPnN][gG]"))

    for img_path in image_paths:
        label_path = Path(label_dir) / (img_path.stem + ".txt")
        if not label_path.exists():
            print(f"Warning: No label found for {img_path.name}")
            continue

        output_path = Path(output_dir) / f"comparison_{img_path.name}"
        predict_and_display(str(img_path), str(label_path), str(output_path))

# Example usage
process_directory("/content/dataset/test/images", "/content/dataset/test/labels", "predictions")